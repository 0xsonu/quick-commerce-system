# Logstash pipeline configuration for ecommerce microservices logs

input {
  # Beats input for Filebeat
  beats {
    port => 5044
  }
  
  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json_lines
  }
  
  # UDP input for syslog
  udp {
    port => 5000
    codec => json_lines
  }
}

filter {
  # Parse container logs from Docker
  if [container] {
    # Extract service name from container name
    if [container][name] {
      grok {
        match => { "[container][name]" => "^/?(?<service_name>[^-]+)(-.*)?$" }
      }
    }
    
    # Add service-specific tags
    if [service_name] {
      mutate {
        add_tag => [ "microservice", "%{service_name}" ]
      }
    }
  }

  # Parse JSON logs from Spring Boot applications
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "parsed_log"
    }
    
    # Extract common fields from parsed JSON
    if [parsed_log] {
      mutate {
        add_field => {
          "log_level" => "%{[parsed_log][level]}"
          "logger_name" => "%{[parsed_log][logger]}"
          "thread_name" => "%{[parsed_log][thread]}"
          "correlation_id" => "%{[parsed_log][correlationId]}"
          "tenant_id" => "%{[parsed_log][tenantId]}"
          "user_id" => "%{[parsed_log][userId]}"
        }
      }
      
      # Handle stack traces
      if [parsed_log][exception] {
        mutate {
          add_field => {
            "exception_class" => "%{[parsed_log][exception][class]}"
            "exception_message" => "%{[parsed_log][exception][message]}"
            "stack_trace" => "%{[parsed_log][exception][stackTrace]}"
          }
          add_tag => [ "exception", "error" ]
        }
      }
      
      # Handle HTTP request logs
      if [parsed_log][http] {
        mutate {
          add_field => {
            "http_method" => "%{[parsed_log][http][method]}"
            "http_path" => "%{[parsed_log][http][path]}"
            "http_status" => "%{[parsed_log][http][status]}"
            "http_duration" => "%{[parsed_log][http][duration]}"
            "http_user_agent" => "%{[parsed_log][http][userAgent]}"
          }
          add_tag => [ "http_request" ]
        }
      }
      
      # Handle database query logs
      if [parsed_log][database] {
        mutate {
          add_field => {
            "db_query_time" => "%{[parsed_log][database][queryTime]}"
            "db_connection_pool" => "%{[parsed_log][database][connectionPool]}"
          }
          add_tag => [ "database" ]
        }
      }
      
      # Handle business metrics
      if [parsed_log][metrics] {
        mutate {
          add_field => {
            "metric_name" => "%{[parsed_log][metrics][name]}"
            "metric_value" => "%{[parsed_log][metrics][value]}"
            "metric_type" => "%{[parsed_log][metrics][type]}"
          }
          add_tag => [ "metrics", "business_metrics" ]
        }
      }
    }
  }
  
  # Parse traditional log formats (fallback)
  else {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:thread_name}\] %{DATA:logger_name} - %{GREEDYDATA:log_message}" 
      }
    }
  }
  
  # Convert timestamp
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
    }
  }
  
  # Add environment information
  mutate {
    add_field => {
      "environment" => "development"
      "cluster" => "local"
    }
  }
  
  # Tag error logs
  if [log_level] == "ERROR" {
    mutate {
      add_tag => [ "error", "alert" ]
    }
  }
  
  # Tag warning logs
  if [log_level] == "WARN" {
    mutate {
      add_tag => [ "warning" ]
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "agent", "ecs", "host", "input" ]
  }
}

output {
  # Output to Elasticsearch with index patterns
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Index pattern based on service and date
    index => "ecommerce-logs-%{service_name:unknown}-%{+YYYY.MM.dd}"
    
    # Document type
    document_type => "_doc"
    
    # Template for index settings
    template_name => "ecommerce-logs"
    template_pattern => "ecommerce-logs-*"
    template => "/usr/share/logstash/templates/ecommerce-logs-template.json"
    template_overwrite => true
    
    # ILM policy
    ilm_enabled => true
    ilm_rollover_alias => "ecommerce-logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "ecommerce-logs-policy"
  }
  
  # Debug output for development
  if "debug" in [tags] {
    stdout {
      codec => rubydebug
    }
  }
  
  # Error handling - send failed events to dead letter queue
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "ecommerce-logs-failed-%{+YYYY.MM.dd}"
    }
  }
}